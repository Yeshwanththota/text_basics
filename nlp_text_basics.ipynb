{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a22fa39",
   "metadata": {
    "papermill": {
     "duration": 0.009908,
     "end_time": "2024-02-02T21:05:54.075623",
     "exception": false,
     "start_time": "2024-02-02T21:05:54.065715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP. Lab 2. Text processing basics\n",
    "\n",
    "In this lab, we will cover a wide range of NLP concepts, including Sentence Segmentation, Lowercasing, Stop Words Removal, Lemmatization, Stemming, Byte-Pair Encoding (BPE), and Edit Distance. Theoretical overviews and practical examples for each concept will be provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3243b",
   "metadata": {
    "papermill": {
     "duration": 0.009046,
     "end_time": "2024-02-02T21:05:54.094079",
     "exception": false,
     "start_time": "2024-02-02T21:05:54.085033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "Sentence segmentation involves breaking down a text into individual sentences, typically separated by punctuation marks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab6166f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:54.114968Z",
     "iopub.status.busy": "2024-02-02T21:05:54.114342Z",
     "iopub.status.idle": "2024-02-02T21:05:56.312830Z",
     "shell.execute_reply": "2024-02-02T21:05:56.311635Z"
    },
    "papermill": {
     "duration": 2.211478,
     "end_time": "2024-02-02T21:05:56.315252",
     "exception": false,
     "start_time": "2024-02-02T21:05:54.103774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample text.', 'It contains multiple sentences.', 'Can we segment it?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"This is a sample text. It contains multiple sentences. Can we segment it?\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58e83f",
   "metadata": {
    "papermill": {
     "duration": 0.009219,
     "end_time": "2024-02-02T21:05:56.334055",
     "exception": false,
     "start_time": "2024-02-02T21:05:56.324836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lowercasing\n",
    "\n",
    "Lowercasing converts all text to lowercase, ensuring uniformity and simplifying text processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc83fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:56.355342Z",
     "iopub.status.busy": "2024-02-02T21:05:56.354470Z",
     "iopub.status.idle": "2024-02-02T21:05:56.360078Z",
     "shell.execute_reply": "2024-02-02T21:05:56.358914Z"
    },
    "papermill": {
     "duration": 0.01901,
     "end_time": "2024-02-02T21:05:56.362534",
     "exception": false,
     "start_time": "2024-02-02T21:05:56.343524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example text.\n"
     ]
    }
   ],
   "source": [
    "text = \"ThIs Is AN ExaMple Text.\"\n",
    "lowercased_text = text.lower()\n",
    "\n",
    "print(lowercased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdb3c8",
   "metadata": {
    "papermill": {
     "duration": 0.009769,
     "end_time": "2024-02-02T21:05:56.382038",
     "exception": false,
     "start_time": "2024-02-02T21:05:56.372269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stop Words Removal\n",
    "\n",
    "Stop words are common words (e.g., \"the,\" \"and\") that are often removed during text processing to focus on meaningful words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501034cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:56.404865Z",
     "iopub.status.busy": "2024-02-02T21:05:56.404179Z",
     "iopub.status.idle": "2024-02-02T21:05:56.639636Z",
     "shell.execute_reply": "2024-02-02T21:05:56.638496Z"
    },
    "papermill": {
     "duration": 0.250527,
     "end_time": "2024-02-02T21:05:56.642060",
     "exception": false,
     "start_time": "2024-02-02T21:05:56.391533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'stop', 'words.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "text = \"This is an example sentence with some stop words.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862c000",
   "metadata": {
    "papermill": {
     "duration": 0.010173,
     "end_time": "2024-02-02T21:05:56.661829",
     "exception": false,
     "start_time": "2024-02-02T21:05:56.651656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form, considering the context and applying morphological analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aedab2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:56.683338Z",
     "iopub.status.busy": "2024-02-02T21:05:56.682188Z",
     "iopub.status.idle": "2024-02-02T21:05:57.510431Z",
     "shell.execute_reply": "2024-02-02T21:05:57.509242Z"
    },
    "papermill": {
     "duration": 0.841694,
     "end_time": "2024-02-02T21:05:57.512999",
     "exception": false,
     "start_time": "2024-02-02T21:05:56.671305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /kaggle/working/...\n",
      "Archive:  /kaggle/working/corpora/wordnet.zip\n",
      "   creating: /kaggle/working/corpora/wordnet/\n",
      "  inflating: /kaggle/working/corpora/wordnet/lexnames  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.verb  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.adv  \n",
      "  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.verb  \n",
      "  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.adj  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.adj  \n",
      "  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n",
      "  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n",
      "  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n",
      "  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n",
      "  inflating: /kaggle/working/corpora/wordnet/README  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.sense  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.noun  \n",
      "  inflating: /kaggle/working/corpora/wordnet/data.adv  \n",
      "  inflating: /kaggle/working/corpora/wordnet/index.noun  \n",
      "  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Download and unzip wordnet\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f679a342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:57.536489Z",
     "iopub.status.busy": "2024-02-02T21:05:57.535591Z",
     "iopub.status.idle": "2024-02-02T21:05:59.844830Z",
     "shell.execute_reply": "2024-02-02T21:05:59.843504Z"
    },
    "papermill": {
     "duration": 2.323962,
     "end_time": "2024-02-02T21:05:59.847413",
     "exception": false,
     "start_time": "2024-02-02T21:05:57.523451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock', 'corpus', 'cry']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"rocks\", \"corpora\", \"cries\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e6ee5",
   "metadata": {
    "papermill": {
     "duration": 0.009898,
     "end_time": "2024-02-02T21:05:59.867227",
     "exception": false,
     "start_time": "2024-02-02T21:05:59.857329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming reduces words to their stems or root form, often by removing suffixes, in a more heuristic approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b77a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:59.889108Z",
     "iopub.status.busy": "2024-02-02T21:05:59.888432Z",
     "iopub.status.idle": "2024-02-02T21:05:59.894586Z",
     "shell.execute_reply": "2024-02-02T21:05:59.893466Z"
    },
    "papermill": {
     "duration": 0.019544,
     "end_time": "2024-02-02T21:05:59.896655",
     "exception": false,
     "start_time": "2024-02-02T21:05:59.877111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'rock', 'beauti']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"rocks\", \"beautifully\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a5113",
   "metadata": {
    "papermill": {
     "duration": 0.009734,
     "end_time": "2024-02-02T21:05:59.916600",
     "exception": false,
     "start_time": "2024-02-02T21:05:59.906866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is a data compression technique used in NLP for tokenization. It breaks down words into subword units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfea844c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:59.938856Z",
     "iopub.status.busy": "2024-02-02T21:05:59.938186Z",
     "iopub.status.idle": "2024-02-02T21:05:59.942671Z",
     "shell.execute_reply": "2024-02-02T21:05:59.941759Z"
    },
    "papermill": {
     "duration": 0.018283,
     "end_time": "2024-02-02T21:05:59.944881",
     "exception": false,
     "start_time": "2024-02-02T21:05:59.926598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72de8025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:05:59.967434Z",
     "iopub.status.busy": "2024-02-02T21:05:59.966504Z",
     "iopub.status.idle": "2024-02-02T21:06:00.034476Z",
     "shell.execute_reply": "2024-02-02T21:06:00.033583Z"
    },
    "papermill": {
     "duration": 0.082093,
     "end_time": "2024-02-02T21:06:00.037042",
     "exception": false,
     "start_time": "2024-02-02T21:05:59.954949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "temp_proc = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d57146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:00.059466Z",
     "iopub.status.busy": "2024-02-02T21:06:00.058750Z",
     "iopub.status.idle": "2024-02-02T21:06:00.071127Z",
     "shell.execute_reply": "2024-02-02T21:06:00.069988Z"
    },
    "papermill": {
     "duration": 0.026654,
     "end_time": "2024-02-02T21:06:00.073890",
     "exception": false,
     "start_time": "2024-02-02T21:06:00.047236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = BPEDecoder()\n",
    "tokenizer.post_processor = temp_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553becff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:00.095692Z",
     "iopub.status.busy": "2024-02-02T21:06:00.095264Z",
     "iopub.status.idle": "2024-02-02T21:06:00.101790Z",
     "shell.execute_reply": "2024-02-02T21:06:00.100950Z"
    },
    "papermill": {
     "duration": 0.020167,
     "end_time": "2024-02-02T21:06:00.104245",
     "exception": false,
     "start_time": "2024-02-02T21:06:00.084078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8015334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:00.127195Z",
     "iopub.status.busy": "2024-02-02T21:06:00.126506Z",
     "iopub.status.idle": "2024-02-02T21:06:00.530145Z",
     "shell.execute_reply": "2024-02-02T21:06:00.529186Z"
    },
    "papermill": {
     "duration": 0.41906,
     "end_time": "2024-02-02T21:06:00.533905",
     "exception": false,
     "start_time": "2024-02-02T21:06:00.114845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download(\"gutenberg\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=5000, special_tokens=special_tokens)\n",
    "shakespeare = [\" \".join(s) for s in gutenberg.sents(\"shakespeare-macbeth.txt\")]\n",
    "tokenizer.train_from_iterator(shakespeare, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a4b0fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:00.566448Z",
     "iopub.status.busy": "2024-02-02T21:06:00.564895Z",
     "iopub.status.idle": "2024-02-02T21:06:00.578308Z",
     "shell.execute_reply": "2024-02-02T21:06:00.577051Z"
    },
    "papermill": {
     "duration": 0.031697,
     "end_time": "2024-02-02T21:06:00.581611",
     "exception": false,
     "start_time": "2024-02-02T21:06:00.549914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'b', 'pe', 'is', 'a', 'd', 'at', 'a', 'com', 'pre', 'ss', 'ion', 'te', 'ch', 'ni', 'que', 'use', 'd', 'in', 'n', 'lp', 'for', 'to', 'ken', 'iz', 'ation', '.', '[SEP]']\n",
      "['[CLS]', 'is', 'this', 'a', 'danger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'handle', 'toward', 'my', 'hand', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.encode(\n",
    "        \"BPE is a data compression technique used in NLP for tokenization.\"\n",
    "    ).tokens\n",
    ")\n",
    "print(\n",
    "    tokenizer.encode(\n",
    "        \"Is this a danger which I see before me, the handle toward my hand?\"\n",
    "    ).tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46650b28",
   "metadata": {
    "papermill": {
     "duration": 0.011517,
     "end_time": "2024-02-02T21:06:00.605405",
     "exception": false,
     "start_time": "2024-02-02T21:06:00.593888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Levenshtein edit distance\n",
    "\n",
    "Edit distance measures the similarity between two strings by counting the minimum number of operations needed to transform one string into the other.\n",
    "\n",
    "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance#Example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b09018a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:00.628377Z",
     "iopub.status.busy": "2024-02-02T21:06:00.627430Z",
     "iopub.status.idle": "2024-02-02T21:06:00.631698Z",
     "shell.execute_reply": "2024-02-02T21:06:00.630936Z"
    },
    "papermill": {
     "duration": 0.017885,
     "end_time": "2024-02-02T21:06:00.633725",
     "exception": false,
     "start_time": "2024-02-02T21:06:00.615840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57bfaaa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:00.656180Z",
     "iopub.status.busy": "2024-02-02T21:06:00.655489Z",
     "iopub.status.idle": "2024-02-02T21:06:01.822560Z",
     "shell.execute_reply": "2024-02-02T21:06:01.821642Z"
    },
    "papermill": {
     "duration": 1.181412,
     "end_time": "2024-02-02T21:06:01.825355",
     "exception": false,
     "start_time": "2024-02-02T21:06:00.643943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance between 'kitten' and 'sitting': 3\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "word1 = \"kitten\"\n",
    "word2 = \"sitting\"\n",
    "distance = Levenshtein.distance(word1, word2)\n",
    "print(f\"Edit distance between '{word1}' and '{word2}': {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73ab0d",
   "metadata": {
    "papermill": {
     "duration": 0.009994,
     "end_time": "2024-02-02T21:06:01.845848",
     "exception": false,
     "start_time": "2024-02-02T21:06:01.835854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4960d2",
   "metadata": {
    "papermill": {
     "duration": 0.010151,
     "end_time": "2024-02-02T21:06:01.866313",
     "exception": false,
     "start_time": "2024-02-02T21:06:01.856162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[Competition](https://www.kaggle.com/t/6dcb6f9def724f9f82050e9092952dd6)\n",
    "\n",
    "The aim of the competition is to count the 10 most frequent words in the plays presented in the `data.txt` file.\n",
    "\n",
    "In order to count the frequent words correctly, you must perform lemmatization and remove stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b17b91bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:01.890206Z",
     "iopub.status.busy": "2024-02-02T21:06:01.888913Z",
     "iopub.status.idle": "2024-02-02T21:06:01.900258Z",
     "shell.execute_reply": "2024-02-02T21:06:01.899485Z"
    },
    "papermill": {
     "duration": 0.025965,
     "end_time": "2024-02-02T21:06:01.902593",
     "exception": false,
     "start_time": "2024-02-02T21:06:01.876628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-caesar.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/kaggle/input/nlp-week-2/data.txt\") as f:\n",
    "    data = f.read()\n",
    "plays = data.split(\"\\n\")\n",
    "plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920737cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:01.925838Z",
     "iopub.status.busy": "2024-02-02T21:06:01.924858Z",
     "iopub.status.idle": "2024-02-02T21:06:01.948630Z",
     "shell.execute_reply": "2024-02-02T21:06:01.947398Z"
    },
    "papermill": {
     "duration": 0.037905,
     "end_time": "2024-02-02T21:06:01.951139",
     "exception": false,
     "start_time": "2024-02-02T21:06:01.913234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt 887071\n",
      "austen-persuasion.txt 466292\n",
      "austen-sense.txt 673022\n",
      "shakespeare-macbeth.txt 100351\n",
      "shakespeare-hamlet.txt 162881\n",
      "shakespeare-caesar.txt 112310\n"
     ]
    }
   ],
   "source": [
    "plays_dict = {}\n",
    "\n",
    "for play in plays:\n",
    "    plays_dict[play] = gutenberg.raw(play)\n",
    "    print(play, len(plays_dict[play]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70acd626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:01.974734Z",
     "iopub.status.busy": "2024-02-02T21:06:01.974242Z",
     "iopub.status.idle": "2024-02-02T21:06:02.052044Z",
     "shell.execute_reply": "2024-02-02T21:06:02.050852Z"
    },
    "papermill": {
     "duration": 0.09251,
     "end_time": "2024-02-02T21:06:02.054451",
     "exception": false,
     "start_time": "2024-02-02T21:06:01.961941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", quiet= True)\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac1e53ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:02.078309Z",
     "iopub.status.busy": "2024-02-02T21:06:02.077928Z",
     "iopub.status.idle": "2024-02-02T21:06:02.087586Z",
     "shell.execute_reply": "2024-02-02T21:06:02.086598Z"
    },
    "papermill": {
     "duration": 0.024166,
     "end_time": "2024-02-02T21:06:02.089862",
     "exception": false,
     "start_time": "2024-02-02T21:06:02.065696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.update(set([',', ';', '.', '!', ':', '@', '#', '--', \"''\", \"``\", \"`\", \"'\", ]))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def top_frequent_words(text, topk=10):\n",
    "    tokens = regexp_tokenize(text.lower(), r'\\w+')\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    filtered_text = [word for word in lemmatized_text if word not in stop_words]\n",
    "    \n",
    "    # Count the occurrences of each word\n",
    "    word_counts = Counter(filtered_text)\n",
    "\n",
    "    # Sort the word counts in descending order\n",
    "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top k elements\n",
    "    return sorted_word_counts[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66b6a327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:02.113704Z",
     "iopub.status.busy": "2024-02-02T21:06:02.113230Z",
     "iopub.status.idle": "2024-02-02T21:06:05.264163Z",
     "shell.execute_reply": "2024-02-02T21:06:05.262916Z"
    },
    "papermill": {
     "duration": 3.165841,
     "end_time": "2024-02-02T21:06:05.266819",
     "exception": false,
     "start_time": "2024-02-02T21:06:02.100978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_words = {}\n",
    "for play, text in plays_dict.items():\n",
    "    top_words[play] = top_frequent_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d5a5cc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T21:06:05.292781Z",
     "iopub.status.busy": "2024-02-02T21:06:05.292333Z",
     "iopub.status.idle": "2024-02-02T21:06:05.299481Z",
     "shell.execute_reply": "2024-02-02T21:06:05.298515Z"
    },
    "papermill": {
     "duration": 0.022662,
     "end_time": "2024-02-02T21:06:05.301818",
     "exception": false,
     "start_time": "2024-02-02T21:06:05.279156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,count\\n\")\n",
    "    for play, counts in top_words.items():\n",
    "        for i, count in enumerate(counts):\n",
    "            f.write(f\"{play}_{i},{count[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2b5eb",
   "metadata": {
    "papermill": {
     "duration": 0.011177,
     "end_time": "2024-02-02T21:06:05.324197",
     "exception": false,
     "start_time": "2024-02-02T21:06:05.313020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7570205,
     "sourceId": 68158,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.29116,
   "end_time": "2024-02-02T21:06:06.260189",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-02T21:05:50.969029",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
